{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "import cv2\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sn\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import csv\n",
    "\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "import tensorflow as tf\n",
    "from PIL import Image\n",
    "\n",
    "import os\n",
    "print(os.listdir(\"../input/stanford-car-dataset-by-classes-folder\"))\n",
    "\n",
    "\n",
    "//\n",
    "\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.applications import densenet\n",
    "from keras.models import Sequential, Model, load_model\n",
    "from keras.layers import Conv2D, MaxPooling2D\n",
    "from keras.layers import Activation, Dropout, Flatten, Dense\n",
    "from keras.callbacks import EarlyStopping, ReduceLROnPlateau, ModelCheckpoint, Callback\n",
    "from keras import regularizers\n",
    "from keras import backend as K\n",
    "K.set_learning_phase(1)\n",
    "img_width, img_height = 224, 224\n",
    "nb_train_samples = 8144\n",
    "nb_validation_samples = 8041\n",
    "epochs = 10\n",
    "batch_size = 32\n",
    "n_classes = 196\n",
    "\n",
    "train_data_dir = path_base + '/car_data/train'\n",
    "validation_data_dir = path_base + '/car_data/test'\n",
    "\n",
    "train_datagen = ImageDataGenerator(\n",
    "    rescale=1. / 255,\n",
    "    #shear_range=0.2,\n",
    "    zoom_range=0.2,\n",
    "    #fill_mode = 'constant',\n",
    "    #cval = 1,\n",
    "    rotation_range = 5,\n",
    "    #width_shift_range=0.2,\n",
    "    #height_shift_range=0.2,\n",
    "    horizontal_flip=True)\n",
    "\n",
    "test_datagen = ImageDataGenerator(rescale=1. / 255)\n",
    "\n",
    "train_generator = train_datagen.flow_from_directory(\n",
    "    train_data_dir,\n",
    "    target_size=(img_width, img_height),\n",
    "    batch_size=batch_size,\n",
    "    class_mode='categorical')\n",
    "\n",
    "validation_generator = test_datagen.flow_from_directory(\n",
    "    validation_data_dir,\n",
    "    target_size=(img_width, img_height),\n",
    "    batch_size=batch_size,\n",
    "    class_mode='categorical')\n",
    "\n",
    "def build_model():\n",
    "    base_model = densenet.DenseNet121(input_shape=(img_width, img_height, 3),\n",
    "                                     weights='../input/full-keras-pretrained-no-top/densenet121_weights_tf_dim_ordering_tf_kernels_notop.h5',\n",
    "                                     include_top=False,\n",
    "                                     pooling='avg')\n",
    "    for layer in base_model.layers:\n",
    "      layer.trainable = True\n",
    "\n",
    "    x = base_model.output\n",
    "    x = Dense(1000, kernel_regularizer=regularizers.l1_l2(0.01), activity_regularizer=regularizers.l2(0.01))(x)\n",
    "    x = Activation('relu')(x)\n",
    "    x = Dense(500, kernel_regularizer=regularizers.l1_l2(0.01), activity_regularizer=regularizers.l2(0.01))(x)\n",
    "    x = Activation('relu')(x)\n",
    "    predictions = Dense(n_classes, activation='softmax')(x)\n",
    "    model = Model(inputs=base_model.input, outputs=predictions)\n",
    "    \n",
    "    return model\n",
    "\n",
    "model = build_model()\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['acc', 'mse'])\n",
    "\n",
    "early_stop = EarlyStopping(monitor='val_loss', patience=8, verbose=1, min_delta=1e-4)\n",
    "reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.1, patience=4, verbose=1, min_delta=1e-4)\n",
    "callbacks_list = [early_stop, reduce_lr]\n",
    "\n",
    "model_history = model.fit_generator(\n",
    "    train_generator,\n",
    "    epochs=epochs,\n",
    "    validation_data=validation_generator,\n",
    "    validation_steps=nb_validation_samples // batch_size,\n",
    "    callbacks=callbacks_list)\n",
    "\n",
    "\n",
    "\n",
    "plt.figure(0)\n",
    "plt.plot(model_history.history['acc'],'r')\n",
    "plt.plot(model_history.history['val_acc'],'g')\n",
    "plt.xticks(np.arange(0, 20, 1.0))\n",
    "plt.rcParams['figure.figsize'] = (8, 6)\n",
    "plt.xlabel(\"Num of Epochs\")\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.title(\"Training Accuracy vs Validation Accuracy\")\n",
    "plt.legend(['train','validation'])\n",
    " \n",
    "plt.figure(1)\n",
    "plt.plot(model_history.history['loss'],'r')\n",
    "plt.plot(model_history.history['val_loss'],'g')\n",
    "plt.xticks(np.arange(0, 20, 1.0))\n",
    "plt.rcParams['figure.figsize'] = (8, 6)\n",
    "plt.xlabel(\"Num of Epochs\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.title(\"Training Loss vs Validation Loss\")\n",
    "plt.legend(['train','validation'])\n",
    "\n",
    "plt.figure(2)\n",
    "plt.plot(model_history.history['mean_squared_error'],'r')\n",
    "plt.plot(model_history.history['val_mean_squared_error'],'g')\n",
    "plt.xticks(np.arange(0, 20, 1.0))\n",
    "plt.rcParams['figure.figsize'] = (8, 6)\n",
    "plt.xlabel(\"Num of Epochs\")\n",
    "plt.ylabel(\"MSE\")\n",
    "plt.title(\"Training Loss vs Validation Loss\")\n",
    "plt.legend(['train','validation'])\n",
    " \n",
    "plt.show()\n",
    "\n",
    "model.evaluate_generator(validation_generator, steps=None, max_queue_size=10, workers=1, use_multiprocessing=False)\n",
    "\n",
    "pred = model.predict_generator(validation_generator, steps=None, max_queue_size=10, workers=1, use_multiprocessing=False, verbose=1)\n",
    "predicted = np.argmax(pred, axis=1)\n",
    "\n",
    "print('Confusion Matrix')\n",
    "cm = confusion_matrix(validation_generator.classes, np.argmax(pred, axis=1))\n",
    "plt.figure(figsize = (30,20))\n",
    "sn.set(font_scale=1.4) #for label size\n",
    "sn.heatmap(cm, annot=True, annot_kws={\"size\": 12}) # font size\n",
    "plt.show()\n",
    "print()\n",
    "print('Classification Report')\n",
    "print(classification_report(validation_generator.classes, predicted, target_names=class_names))\n",
    "\n",
    "def predict_one(model):\n",
    "    image_batch, classes_batch = next(validation_generator)\n",
    "    predicted_batch = model.predict(image_batch)\n",
    "    for k in range(0,image_batch.shape[0]):\n",
    "      image = image_batch[k]\n",
    "      pred = predicted_batch[k]\n",
    "      the_pred = np.argmax(pred)\n",
    "      predicted = class_names[the_pred]\n",
    "      val_pred = max(pred)\n",
    "      the_class = np.argmax(classes_batch[k])\n",
    "      value = class_names[np.argmax(classes_batch[k])]\n",
    "      plt.figure(k)\n",
    "      isTrue = (the_pred == the_class)\n",
    "      plt.title(str(isTrue) + ' - class: ' + value + ' - ' + 'predicted: ' + predicted + '[' + str(val_pred) + ']')\n",
    "      plt.imshow(image)\n",
    "\n",
    "predict_one(model)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
